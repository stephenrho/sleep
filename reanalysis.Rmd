---
title: "No clear or consistent evidence that wearing an eye mask leads to meaningful improvement in learning and alertness: A reanalysis of Greco et al. (2023)"
author: "Stephen Rhodes^[steverho89@gmail.com. Journal version (paywalled): https://doi.org/10.1093/sleep/zsad105]"
date: 'Last updated: `r Sys.Date()`'
output:
  pdf_document: default
---

Greco et al.$^{[1]}$, in the "Statement of significance" section, claim that their findings suggest that "[...] wearing an eye mask during sleep is an effective, economical, and noninvasive behavior that could benefit cognitive function and lead to measurable impacts on everyday life." Here we examine some of the analysis choices made in this article and critically assess this claim. Firstly, Greco et al. should be applauded for clearly stating their analysis model and for publicly sharing their data (https://osf.io/q4p9v/), which made this reanalysis possible.

In Experiment 1, paired associates learning (PAL) was assessed on day 6 and 7 of the manipulation (eye mask or control), following 5 nights of habituation. On day 6 participants learned words paired to particular cue words up to a 60% correct criterion. The first test of PAL was then 10 minutes later with the number of correctly recalled words (out of 80) being the outcome. The second PAL assessment occurred on day 7 with no additional learning opportunities in between. The two PAL assessments were split up and analyzed separately via the following mixed effects model (using syntax from the `lme4` package for `R`):
```
lmer(y ~ eye_mask + (1 | ID) + (1 | year), data = data, REML = FALSE)
```
where `y` is the outcome of interest, `year` was a factor with two levels representing changes in the nature of the control condition from 2018 to 2019, and `eye_mask` refers to whether or not participants had worn an eye mask (1 = yes, 0 = no) in the 5 or 6 nights prior to testing. 

Using this model there was a significant difference in the average number of words recalled between mask and control on day 6. However, these conditions did not differ in terms of an 'absolute consolidation' score, which was the difference in performance between days 6 and 7 (reported in the supplement).[^msl]

[^msl]: It is interesting to note that a similar overnight change analysis is presented for the motor skill learning task in the supplement but the main article reports an analysis in which the interaction of condition and day was tested for this outcome.

It is not clear why the day 6 and 7 results were separated, as performance on the PAL task could have been assessed via a single model. Indeed a single model analysis is preferable if the difference between immediate and delayed recall is of interest. Splitting the data to assess a 'mask effect' obviously inflates the type i error rate. A more appropriate model would have also allowed for an interaction between `year` and `eye_mask` as changing the nature of the control condition may have influenced the *difference* between the control and eye mask conditions, which is encoded in the fixed effect for `eye_mask`. Including `year` as a random effect only allows for overall performance to vary by `year` of study. Further, as year only has two levels, it seems inappropriate to try to estimate a variance component for this factor and, as we found when recreating the analysis, there were situations where this caused fit issues in the reported analysis (the 'singular fit' warning from `lme4`).

As participants were assessed under both the control and eye mask conditions, assessing the PAL data as a whole allows one to model between participant variability in the size of the eye mask effect (i.e., a random slope term for `eye_mask`). Not accounting for this source of variability in the model is important as the standard errors for the fixed effect of `eye_mask` will be too small, leading to incorrect p-values, and, potentially, incorrect decisions as to whether to reject the null hypothesis of no difference.$^{[2]}$

We reanalyzed the PAL data with the following model, which allows for an interaction between condition (`eye_mask`) and year, to account for the different control conditions, and an interaction between condition and day, which codes whether the assessment was on day 6 (immediate) or day 7 (delayed).[^reml]
```
 lmer(y ~ eye_mask + year + day + eye_mask:year + eye_mask:day + 
             (1 + eye_mask + day | ID), 
           data = pal1, REML = FALSE) 
```

[^reml]: We have set `REML=FALSE` to fit the model via maximum likelihood, rather than restricted maximum likelihood (REML). This is to match Greco et al., who used ML to allow for a likelihood ratio test. However, `REML=TRUE` would likely lead to better estimates of fixed effect standard errors.

In this analysis the coefficient for `eye_mask` is not significantly different from zero ($b$ = 0.99 [-0.01, 2.00], $p$ = 0.053).[^stz] Further, the interactions between condition and year or day are not significant. There is also no significant eye mask vs control condition main effect in a mixed models ANOVA, which, unlike the above model, includes all two way and the three way interactions ($F$(1, 81) = 3.77, $p$ = 0.056). 

[^stz]: `year` and `day` were 'sum-to-zero' coded so that the `eye_mask` fixed effect can be directly interpreted as an 'overall' eye mask effect.

We also reanalyzed average reaction times from the psychomotor vigilance test (PVT) via a mixed model that allowed for a fixed effect interaction between year and condition, as well as a participant-level random intercept and slope for the difference between eye mask and control. The difference between conditions is significantly different from zero, albeit with greater uncertainty in the magnitude of the difference, $b$ = -6.239 [-12.363, -0.115], $p$ = 0.047.

For the data from Experiment 2, it is not possible to estimate a random participant slope for eye mask as there was only one score per condition. Greco et al. also note that this experiment was not powered to detect differences in PAL or PVT scores, as the goal was to assess correlations with electrophysiological measures (see supplement to article). Nevertheless, we recreated the results for both tasks but found that the number of unique participant IDs in the data files was lower (N = 28) than that reported in the article (N = 33 following exclusions).

So what are we to make of the effects of eye masks on cognition? When reassessing the data presented with more appropriate models we find less consistent evidence for cognitive effects of wearing an eye mask. However, this reanalysis should also be interpreted with caution. Information on condition order was not available in the data files and is potentially important given that some participants were omitted from analysis, thereby possibly leading to an imbalanced design. Further, the participant-level data from the PVT are average reaction times from an unspecified number of trials where responses could be missing due to overly short or long response times. Trial level data would allow one to take into account these additional sources of variation/uncertainty. 

Even if we were to take these improved estimates at face value it is difficult to imagine how being able to, on average, remember 0 to 2 (the 95% CI around the `eye_mask` effect) extra words out of 80 or being able to respond 0.1 to 12 milliseconds faster after wearing an eye mask could lead to measurable impacts on everyday life.

# Response to the Response

Greco et al. responded to this commentary (https://doi.org/10.1093/sleep/zsad148) and I thank them for taking the time to do so. Here are responses to some specific points:

- The first paragraph gives a rationale for splitting day 6 and 7 scores for the PAL task. I don't dispute the logic but what isn't addressed is the obvious effect this has on the type I error rate for the test of 'mask effects on memory performance'. Also not addressed is the point (in the first footnote above) that a similar analysis of change is presented in the supplement for the motor learning task but a different analysis, including both day 6 and 7 data, was reported in the main paper. Therefore, we know that at least this task was looked at multiple ways. This doesn't mean that all tasks were analyzed multiple ways (researcher degrees of freedom) but preregistration would have been useful here.
- Greco et al. point out that the year\*eye_mask interaction does not significantly improve model fit according to a likelihood ratio test. This isn't really a surprise given that the coefficient wasn't significant in the reanalysis. I would argue that the interaction should be included *irrespective* of whether it significantly improves model fit. This is to account for even slight variability in the size of any 'mask effect' by year. Not including this in the model fixes it to be zero and that seems a strong assumption (and one not justifiable on the basis of p > 0.05).
- The reanalysis of the PVT task is said to have strengthened confidence in that finding. However, what is not addressed is the point that raw data for the PVT task are not available on the OSF. Not accounting for trial level variability in response times and variability in the number of responses available means that the standard errors (and, therefore, p-values) are likely too small. This should not strengthen confidence.
- In the final paragraph Greco et al. argue that being able to respond even a few milliseconds faster or remember one or two extra words *could* have huge implications. I suppose... but this focuses on the point estimates and ignores the fact that the 95% confidence intervals include differences that any rational reader would consider meaningless. Establishing the impact of wearing eye masks during sleep on cognition is clearly going to take more work (and more convincing treatment effect estimates). It also takes a large leap to assume that an RT difference in a lab task would translate to a similar RT difference in a 'real world' scenario, like driving.
- The above point only matters if we take the estimates seriously. As argued in the letter above, there are participants omitted from analysis, for various reasons, leading to the possibility that the design is imbalanced. If, for example, more participants completed the eye mask condition second then some/all of the performance difference could reflect practice, having done the tasks before. This, surprisingly, was not addressed in the response.

# References

[1] Greco, V., Bergamo, D., Cuoccio, P., Konkoly, K. R., Mu√±oz Lombardo, K., & Lewis, P. A. (2023). Wearing an eye mask during overnight sleep improves episodic learning and alertness. Sleep, 46(3), zsac305.

[2] Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. *Journal of memory and language, 68*(3), 255-278.

# Code and R output

Additional code for results not presented here is available at https://github.com/stephenrho/sleep/blob/main/reanalysis.Rmd

```{r setup, include=T}
knitr::opts_chunk$set(echo = T, warning = F, message = F)

library(data.table)
library(ggplot2)
library(lme4)
library(lmerTest)
library(afex)

theme_set(theme_bw())

# get the data
proj_url = "https://osf.io/q4p9v/"

if (!dir.exists("data/")){
  library(osfr)
  dir.create("data/")
  osf = osf_retrieve_node(proj_url)
  osf_download(osf_ls_files(osf), recurse = T, path = "data/") 
}

readdat <- function(file){
  # reshape the data
  exp = ifelse(grepl("Experiment1", file), 1, 2)
  
  if (exp == 1){
    dat = readxl::read_xlsx(file, skip = 1)
    
    stopifnot(all(colnames(dat) == c('Participants', 'YearOfExperiment', 
                                     'Eyemask...3', 'ControlMask...4', 
                                     'Eyemask...5', 'ControlMask...6')))
    
    dv_cols = paste(
      rep(c("Eyemask", "Control"), 2), 
      rep(c("day6", "day7"), each = 2), 
      sep = "_"
    )
    
    colnames(dat)[3:6] = dv_cols
  } else{
    dat = readxl::read_xlsx(file)
    
    stopifnot(all(colnames(dat) == c('Participants', 'YearOfExperiment', 
                                     'Eyemask', 'ControlMask')))
    dv_cols = c("Eyemask", "Control")
    colnames(dat)[3:4] = dv_cols
  }
  
  colnames(dat)[2] = "year"
  
  dat$ID = paste(dat$Participants, dat$year, sep = "_")
  # wide to long
  d = reshape2::melt(dat, id.vars = c("ID", "year"), measure.vars = dv_cols, variable.name = "condition", value.name = "y")
  
  d$eye_mask = as.numeric(grepl("Eyemask", d$condition))
  
  if (exp == 1){
    d$day = ifelse(grepl("day7", d$condition), "day 7", "day 6")
    d$day = as.factor(d$day)
    d$condition = gsub("_day6|_day7", "", d$condition)
  }
  
  d$condition = as.factor(d$condition)
  d$year = as.factor(d$year)
  
  return(d)
}

pal1 = readdat("data/Tasks&Questionnaire/PAL/PAL_Experiment1.xlsx")
pvt1 = readdat("data/Tasks&Questionnaire/PVT/PVT_Experiment1.xlsx")
msl1 = readdat("data/Tasks&Questionnaire/MSL/MSL_Experiment1.xlsx")

pal2 = readdat("data/Tasks&Questionnaire/PAL/PAL_Experiment2.xlsx")
pvt2 = readdat("data/Tasks&Questionnaire/PVT/PVT_Experiment2.xlsx")
```

## Paired associates learning

```{r PAL}
### PAL (primary outcome) ----
length(unique(pal1$ID))

ggplot(pal1, aes(x = condition, y=y, group=ID)) + 
  geom_line(alpha=1/2) + 
  stat_summary(aes(x = condition, y=y, group=1), 
               fun="mean", geom="line", inherit.aes = F, 
               lwd=3, col="red") +
  #facet_wrap(~day) + 
  facet_grid(year ~ day) +
  labs(x="Condition", y="PAL score", title="Experiment 1")

aggregate(y ~ eye_mask, data = subset(pal1, day == "day 6"), FUN = mean)
aggregate(y ~ eye_mask, data = subset(pal1, day == "day 7"), FUN = mean)
aggregate(y ~ eye_mask, data = pal1, FUN = mean)

# original model
lmer(y ~ eye_mask + (1 | ID) + (1 | year), data = subset(pal1, day == "day 6"), 
     REML = FALSE) |>
  summary()

# analysis reported in supplement
pal1w = reshape2::dcast(pal1, formula = ... ~ day, value.var = "y")
pal1w$y = pal1w$`day 6` - pal1w$`day 7`

lmer(y ~ eye_mask + (1 | ID) + (1 | year), data = pal1w, 
     REML = FALSE) |>
  summary()

# note: in both cases there is a singular fit warning
# most likely trying to estimate random effect for 2 groups...

# more appropriate model
# (1) analyze the data from the PAL task as a whole.
# no need to separate days.
# (2) account for ID level variability in difference 
# between eye mask and control
# (3) year = different control conditions, so should account 
# for *interaction* between eye mask and year 
# doesn't make sense to include year as random effect

# make contrast for year and day sum to zero so coefficient 
# for mask is at 'average' year
contrasts(pal1$year) = c(-1,1)
contrasts(pal1$day) = c(-1,1) 
mod = lmer(y ~ eye_mask + year + day + eye_mask:year + eye_mask:day + 
             (1 + eye_mask + day | ID), 
           data = pal1, REML = FALSE) 

summary(mod)
confint(mod)
# some warnings but profile plots/zeta diagrams look ok 
# https://stackoverflow.com/questions/74018300/warnings-when-computing-confidence-intervals-using-confint-for-linear-growth-mod
# pp = profile(mod)
# lattice::xyplot(pp)

# mixed ANOVA (note: tests extra interactions)
aov_car(y ~ condition*year*day + Error(ID/(condition*day)), 
        data = pal1, fun_aggregate = mean)

# Experiment 2
# n unique IDs don't match N = 33 reported in paper
length(unique(pal2$ID))

aggregate(y ~ condition, pal2, mean)
# the means match though...

ggplot(pal2, aes(x = condition, y=y, group=ID)) + 
  geom_line(alpha=1/2) + 
  stat_summary(aes(x = condition, y=y, group=1), 
               fun="mean", geom="line", inherit.aes = F, 
               lwd=3, col="red") +
  labs(x="Condition", y="PAL score", title="Experiment 2")

# recreate original analysis
mod = lmer(y ~ eye_mask + (1  | ID), data = pal2, REML = F)

summary(mod)
confint(mod)

aov_car(y ~ condition + Error(ID/condition), data = pal2, fun_aggregate = mean)

```

## Psychomotor vigilance test

```{r PVT}
### PVT ----
length(unique(pvt1$ID))

ggplot(pvt1, aes(x = condition, y=y, group=ID)) + 
  geom_line(alpha=1/2) + 
  stat_summary(aes(x = condition, y=y, group=1), 
               fun="mean", geom="line", inherit.aes = F, 
               lwd=3, col="red") +
  #facet_wrap(~day) + 
  facet_grid(year ~ day) +
  labs(x="Condition", y="PVT RT (ms)", title="Experiment 1")

# this matches reported means (but figure 2b means look different?)
aggregate(y ~ eye_mask, data = pvt1, FUN = mean)

# original model
lmer(y ~ eye_mask + (1 | ID) + (1 | year), data = pvt1, REML = FALSE) |>
  summary()
# note: boundary (singular) fit: see help('isSingular') 

# more appropriate model
contrasts(pvt1$year) = c(-1,1)
mod = lmer(y ~ eye_mask*year + (1 + eye_mask | ID), data = pvt1, REML = FALSE) 

summary(mod)
confint(mod)

# mixed ANOVA (averages day 6 and 7)
aov_car(y ~ condition*year + Error(ID/condition), 
        data = pvt1, fun_aggregate = mean)

# Experiment 2
# also 28, not 33
length(unique(pvt2$ID))

ggplot(pvt2, aes(x = condition, y=y, group=ID)) + 
  geom_line(alpha=1/2) + 
  stat_summary(aes(x = condition, y=y, group=1), 
               fun="mean", geom="line", inherit.aes = F, 
               lwd=3, col="red") +
  labs(x="Condition", y="PVT RT (ms)", title="Experiment 2")

aggregate(y ~ condition, pvt2, mean)

aov_car(y ~ condition + Error(ID/condition), data = pvt2, fun_aggregate = mean)

lmer(y ~ eye_mask + (1  | ID), data = pvt2, REML = F) |>
  summary()

```

```{r MSL, include=F}
# Motor-Skill Learning

# change include to T above to include this analysis

### MSL ----
length(unique(msl1$ID))

ggplot(msl1, aes(x = condition, y=y, group=ID)) + 
  geom_line(alpha=1/2) + 
  stat_summary(aes(x = condition, y=y, group=1), 
               fun="mean", geom="line", inherit.aes = F, 
               lwd=3, col="red") +
  #facet_wrap(~day) + 
  facet_grid(year ~ day) +
  labs(x="Condition", y="MSL Score", title="Experiment 1")

aggregate(y ~ eye_mask, data = msl1, FUN = mean)

# original model
lmer(y ~ eye_mask + (1 | ID) + (1 | year), data = msl1, REML = FALSE) |>
  summary()

# actually this is the model reported...
lmer(y ~ eye_mask*day + (1 | ID) + (1 | year), data = msl1, REML = FALSE) |>
  summary()
# interpretation in the paper is that people get 
# better after sleep. Also probably practice

# more appropriate model
lmer(y ~ eye_mask*year + (1 + eye_mask | ID), data = msl1, REML = FALSE) |>
  summary()

# mixed ANOVA (averages day 6 and 7)
aov_car(y ~ condition*year + Error(ID/condition), data = msl1, fun_aggregate = mean)
```

